# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03b_multi_core.learner.ipynb (unless otherwise specified).

__all__ = ['pack_learner_args', 'reload_child_model']

# Internal Cell
from ..utils import xla_imported

# Internal Cell
# import sys
# def xla_imported():
#     return 'torch_xla' in sys.modules

# Internal Cell
try:
    import torch_xla
except ImportError:
    pass

# Internal Cell
from . import *
from ..misc_utils import *

# Cell
def _make_xla_child_learner(rank, sync_valid,learner_args):
    sync_valid = True
    device = xm.xla_device()
    world_size = xm.xrt_world_size()
    dls = make_distributed_dataloaders(learner_args.pop('base_dls'),
                                       rank, world_size, sync_valid=sync_valid)

    model = learner_args.pop('wrapped_model').to(device)

    learner = Learner(dls, model,**learner_args)
    learner.to_xla(device, rank, sync_valid=sync_valid)
    return learner



# Cell
def _xla_run_fit(rank, learner_args, fit_args):
    sync_valid = True
    learner = _make_xla_child_learner(rank, sync_valid, learner_args)
    learner.fit(**fit_args)
    learner.save('_xla_tmp_model')
    xm.mark_step()

# Cell
def _xla_run_fit_one_cycle(rank, learner_args, fit_args):
    sync_valid = True
    learner = _make_xla_child_learner(rank, sync_valid, learner_args)
    learner.fit_one_cycle(**fit_args)
    learner.save('_xla_tmp_model')
    xm.mark_step()

# Cell
from fastcore.basics import defaults, patch_to, patch
from fastai.learner import Learner
from fastai.callback.progress import ProgressCallback
@patch_to(Learner)
def pack_learner_args(self):
    learner_args = {}
    learner_args['wrapped_model'] =  xmp.MpModelWrapper(self.model)
    learner_args['base_dls'] = self.dls
    learner_args['opt_func'] = self.opt_func
    learner_args['loss_func'] = self.loss_func
    learner_args['metrics'] = self.metrics
    # fetch only cbs not in defaults
    if ProgressCallback not in defaults.callbacks:
        defaults.callbacks.append(ProgressCallback)
    learner_args['cbs'] = [cb for cb in self.cbs
                      if cb.name not in L(defaults.callbacks).attrgot('name')]

    learner_args['wd'] = self.wd
    learner_args['moms'] = self.moms
    learner_args['lr'] = self.lr
    learner_args['splitter'] = self.splitter
    learner_args['path'] = self.path
    learner_args['model_dir'] = self.model_dir
    learner_args['wd_bn_bias'] = self.wd_bn_bias
    learner_args['train_bn'] = self.train_bn
    return learner_args

# Cell
@patch_to(Learner)
def reload_child_model(self):
    # blatantly stolen from fastai LRFinder after_fit :)
    tmp_f = self.path/self.model_dir/'_xla_tmp_model.pth'
    if tmp_f.exists():
        self.opt.zero_grad()
        self.load('_xla_tmp_model', with_opt=False)
        os.remove(tmp_f)
        self.create_opt()

# Cell

from fastcore.meta import delegates
@patch
@delegates(Learner.fit, but='num_cores')
def xla_fit(self:Learner, n_epoch, num_cores=8, **kwargs):
    """call fit in multicore tpu environment"""
    self.remove_cbs(ProgressCallback)
    learner_args = self.pack_learner_args()
    fit_args={**kwargs}
    fit_args['n_epoch'] = n_epoch
    xmp.spawn(_xla_run_fit,
              args=(learner_args, fit_args,),
              nprocs=num_cores,
              start_method='fork')
    self.reload_child_model()
    self.add_cbs(ProgressCallback)


# Cell
from fastai.learner import Learner
from fastai.callback.schedule import *
@patch
@delegates(Learner.fit_one_cycle, but='num_cores')
def xla_fit_one_cycle(self:Learner, n_epoch, num_cores=8, **kwargs):
    """call fit_one_cycle in multicore tpu environment"""
    self.remove_cbs(ProgressCallback)
    learner_args = self.pack_learner_args()
    fit_args={**kwargs}
    fit_args['n_epoch'] = n_epoch
    xmp.spawn(_xla_run_fit_one_cycle,
              args=(learner_args, fit_args,),
              nprocs=num_cores,
              start_method='fork')
    self.reload_child_model()
    self.add_cbs(ProgressCallback)